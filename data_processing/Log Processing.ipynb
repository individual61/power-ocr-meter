{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e2bb412-6fe2-4cdf-85ab-6f978e062b18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T02:14:33.093716Z",
     "iopub.status.busy": "2025-09-03T02:14:33.093216Z",
     "iopub.status.idle": "2025-09-03T02:14:34.499875Z",
     "shell.execute_reply": "2025-09-03T02:14:34.499375Z",
     "shell.execute_reply.started": "2025-09-03T02:14:33.093716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 32 CSV files; total rows: 234375\n",
      "File spans collected: 32 (earliest 2025-08-18 12:41:45.321000 → latest 2025-08-29 15:41:16.761000)\n",
      "[Step 1] Applied cutoff at 2025-08-21: kept 230935 / 234375 rows\n",
      "[Step 2] Discarded non-'watt' modes: 3869; rows remaining: 227066\n",
      "[Step 3] Discarded rows where value contains '8888.8000': 0; rows remaining: 227066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paul\\AppData\\Local\\Temp\\ipykernel_49496\\3372660254.py:122: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  err_decode_mask = df[\"error\"].str.contains(r\"(decode|segment|unrecognized|fail|parse)\", case=False, na=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4] Discarded rows with decode/segment warnings AND zero/NaN value: 11760; rows remaining: 215306\n",
      "\n",
      "CSV files represented (post-cleaning): 23\n",
      "Segment-decode error rows (remaining): 0 (0.00% of total)\n",
      "Time span: 2025-08-22 16:37:50.372000 → 2025-08-29 15:37:08.748000\n",
      "Total runtime: 6 days, 22 hours, 59 minutes\n"
     ]
    }
   ],
   "source": [
    "# Starter cell: load logs, apply cleaning steps, compute file spans, and report stats\n",
    "# Adjust DATA_DIR if your logs live somewhere else.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Load all CSV logs (read as strings first so we can do exact/substring checks)\n",
    "# Also collect per-file time spans for plotting generator activity later.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "DATA_DIR = Path(\"../data-2025\")\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Data directory not found: {DATA_DIR.resolve()}\")\n",
    "\n",
    "csv_paths = sorted(DATA_DIR.rglob(\"*.csv\"))\n",
    "\n",
    "frames = []\n",
    "spans = []  # collect per-file spans: {\"source_file\", \"start\", \"end\"}\n",
    "expected_cols = [\"timestamp\",\"mode\",\"value\",\"vbat_mV\",\"vin_mV\",\"iout_mA\",\"soc_C\",\"rp1_C\",\"pmic_C\",\"error\"]\n",
    "\n",
    "for p in csv_paths:\n",
    "    try:\n",
    "        tmp = pd.read_csv(p, dtype=str)   # keep everything as str for now\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: failed to read {p}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Ensure expected columns exist\n",
    "    for col in expected_cols:\n",
    "        if col not in tmp.columns:\n",
    "            tmp[col] = np.nan\n",
    "\n",
    "    # Preserve raw value string EXACTLY as read (no strip/normalize yet)\n",
    "    tmp[\"value_str\"] = tmp[\"value\"].astype(str)\n",
    "\n",
    "    # Normalize text fields we care about\n",
    "    tmp[\"mode\"]  = tmp[\"mode\"].astype(str).str.strip().str.lower()\n",
    "    tmp[\"error\"] = tmp[\"error\"].astype(str).fillna(\"\").str.strip()\n",
    "\n",
    "    # Parse timestamp\n",
    "    tmp[\"timestamp\"] = pd.to_datetime(tmp[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "    # Record this file's span (based on whatever timestamps exist in the file)\n",
    "    valid_ts = tmp[\"timestamp\"].dropna()\n",
    "    if not valid_ts.empty:\n",
    "        spans.append({\n",
    "            \"source_file\": str(p),\n",
    "            \"start\": valid_ts.min(),\n",
    "            \"end\":   valid_ts.max(),\n",
    "        })\n",
    "\n",
    "    # Keep source filename\n",
    "    tmp[\"source_file\"] = str(p)\n",
    "\n",
    "    frames.append(tmp)\n",
    "\n",
    "if frames:\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    df = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
    "else:\n",
    "    df = pd.DataFrame(columns=expected_cols + [\"value_str\",\"source_file\"])\n",
    "\n",
    "# Build spans DataFrame (raw)\n",
    "log_spans = pd.DataFrame(spans) if spans else pd.DataFrame(columns=[\"source_file\",\"start\",\"end\"])\n",
    "log_spans = log_spans.sort_values([\"start\",\"end\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded {len(csv_paths)} CSV files; total rows: {len(df)}\")\n",
    "if not log_spans.empty:\n",
    "    print(f\"File spans collected: {len(log_spans)} (earliest {log_spans['start'].min()} → latest {log_spans['end'].max()})\")\n",
    "else:\n",
    "    print(\"No file spans collected.\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# STEP 1: Apply cutoff date; report rows remaining\n",
    "# Also clip the file spans to the same cutoff, producing log_spans_cut.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "CUTOFF_DATE = pd.Timestamp(\"2025-08-21\")  # inclusive: keep rows >= 2025-08-21 00:00\n",
    "before_rows = len(df)\n",
    "df = df[df[\"timestamp\"] >= CUTOFF_DATE].copy()\n",
    "print(f\"[Step 1] Applied cutoff at {CUTOFF_DATE.date()}: kept {len(df)} / {before_rows} rows\")\n",
    "\n",
    "# Clip spans to cutoff (keep only those that overlap the cutoff window)\n",
    "if not log_spans.empty:\n",
    "    log_spans_cut = log_spans.copy()\n",
    "    # drop spans that end before cutoff\n",
    "    log_spans_cut = log_spans_cut[log_spans_cut[\"end\"] >= CUTOFF_DATE].copy()\n",
    "    # clip start to cutoff\n",
    "    log_spans_cut[\"start\"] = log_spans_cut[\"start\"].where(log_spans_cut[\"start\"] >= CUTOFF_DATE, CUTOFF_DATE)\n",
    "    # ensure start < end\n",
    "    log_spans_cut = log_spans_cut[log_spans_cut[\"start\"] < log_spans_cut[\"end\"]].reset_index(drop=True)\n",
    "else:\n",
    "    log_spans_cut = log_spans.copy()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# STEP 2: Keep only rows where mode is exactly \"watt\"; report discarded & remaining\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "pre_rows = len(df)\n",
    "mode_mask = df[\"mode\"].eq(\"watt\")   # exact string match\n",
    "discarded_non_watt = int((~mode_mask).sum())\n",
    "df = df[mode_mask].copy()\n",
    "print(f\"[Step 2] Discarded non-'watt' modes: {discarded_non_watt}; rows remaining: {len(df)}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# STEP 3: Discard rows whose raw 'value' STRING contains \"8888.8000\"\n",
    "#         (substring match; no numeric conversions; resilient to quotes/padding)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "pre_rows = len(df)\n",
    "sentinel_mask = df[\"value_str\"].astype(str).str.contains(\"8888.8000\", regex=False, na=False)\n",
    "discarded_sentinels = int(sentinel_mask.sum())\n",
    "df = df[~sentinel_mask].copy()\n",
    "print(f\"[Step 3] Discarded rows where value contains '8888.8000': {discarded_sentinels}; rows remaining: {len(df)}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# STEP 4: Discard rows with decode/segment warnings *AND* zero/NaN reading\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "pre_rows = len(df)\n",
    "value_norm_for_num = df[\"value_str\"].str.replace(r\"[^\\d.\\-]\", \"\", regex=True)\n",
    "value_num_tmp = pd.to_numeric(value_norm_for_num, errors=\"coerce\")\n",
    "zero_or_nan = value_num_tmp.isna() | value_num_tmp.eq(0)\n",
    "err_decode_mask = df[\"error\"].str.contains(r\"(decode|segment|unrecognized|fail|parse)\", case=False, na=False)\n",
    "drop_mask = err_decode_mask & zero_or_nan\n",
    "discarded_err_zero = int(drop_mask.sum())\n",
    "df = df[~drop_mask].copy()\n",
    "print(f\"[Step 4] Discarded rows with decode/segment warnings AND zero/NaN value: {discarded_err_zero}; rows remaining: {len(df)}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Additional summary (post-cleaning)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Convert numerics now that string-based filtering is done\n",
    "for num_col in [\"value\",\"vbat_mV\",\"vin_mV\",\"iout_mA\",\"soc_C\",\"rp1_C\",\"pmic_C\"]:\n",
    "    df[num_col] = pd.to_numeric(df[num_col], errors=\"coerce\")\n",
    "\n",
    "# Decode-error count & percentage (post-cleaning; informational only)\n",
    "pat = re.compile(r\"(?:decode.*digit|segment)\", re.IGNORECASE)\n",
    "decode_err_mask = df[\"error\"].str.contains(pat, na=False)\n",
    "decode_err_rows = int(decode_err_mask.sum())\n",
    "total_rows = len(df)\n",
    "decode_pct = (decode_err_rows / total_rows * 100.0) if total_rows else 0.0\n",
    "\n",
    "# Single total runtime as D/H/M\n",
    "if total_rows and df[\"timestamp\"].notna().any():\n",
    "    tmin = df[\"timestamp\"].min()\n",
    "    tmax = df[\"timestamp\"].max()\n",
    "    span = tmax - tmin\n",
    "    total_seconds = int(span.total_seconds())\n",
    "    days = total_seconds // 86400\n",
    "    rem = total_seconds % 86400\n",
    "    hours = rem // 3600\n",
    "    rem %= 3600\n",
    "    minutes = rem // 60\n",
    "else:\n",
    "    tmin = tmax = None\n",
    "    days = hours = minutes = 0\n",
    "\n",
    "print(f\"\\nCSV files represented (post-cleaning): {df['source_file'].nunique()}\")\n",
    "print(f\"Segment-decode error rows (remaining): {decode_err_rows} ({decode_pct:.2f}% of total)\")\n",
    "print(f\"Time span: {tmin} → {tmax}\")\n",
    "print(f\"Total runtime: {days} days, {hours} hours, {minutes} minutes\")\n",
    "\n",
    "# Globals for the chart cell:\n",
    "# - df: cleaned rows for plotting values\n",
    "# - log_spans_cut: file intervals (clipped to cutoff) for generator-on overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55facdcb-1783-40f3-8884-84c5a337028e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T04:42:51.938548Z",
     "iopub.status.busy": "2025-09-03T04:42:51.938048Z",
     "iopub.status.idle": "2025-09-03T04:42:52.644591Z",
     "shell.execute_reply": "2025-09-03T04:42:52.644091Z",
     "shell.execute_reply.started": "2025-09-03T04:42:51.938548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved power_watts_all_with_spans.png\n"
     ]
    }
   ],
   "source": [
    "# Chart: Watts vs Time (cleaned df) with generator intervals overlay\n",
    "# - Power plotted as LINES (no markers), but ONLY within logfile spans (no bridging across gaps)\n",
    "# - Thick red line at y=0 for each CSV file span (from log_spans_cut)\n",
    "# - 2000 x 1000 PNG, daily major ticks (\"Friday 8/29\") LEFT-ALIGNED, faint hourly grid, night shading\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# ───────────── CONFIG ─────────────\n",
    "PLOT_SCOPE = \"all\"      # \"all\" or \"subset\"\n",
    "SUBSET_STYLE = \"window\"  # \"window\" or \"around\"\n",
    "\n",
    "# (A) Time window:\n",
    "SUBSET_START = \"2025-08-25 18:00:00\"\n",
    "SUBSET_END   = \"2025-08-25 21:00:00\"\n",
    "\n",
    "# (B) Time around a center point (± minutes):\n",
    "CENTER_TIME  = \"2025-08-25 19:05:00\"\n",
    "RADIUS_MIN   = 10\n",
    "\n",
    "OUT_PNG = \"power_watts_all_with_spans.png\" if PLOT_SCOPE == \"all\" else \"power_watts_subset_with_spans.png\"\n",
    "PRINT_MAX_ROWS = 500\n",
    "SAVE_SUBSET_CSV = True\n",
    "SUBSET_CSV_PATH = \"subset_rows_for_manual_check.csv\"\n",
    "\n",
    "SUNRISE_STR = \"06:22:28\"\n",
    "SUNSET_STR  = \"19:31:14\"\n",
    "\n",
    "FIGSIZE_IN = (30, 5)  # 2000x1000 at 200 DPI\n",
    "DPI = 200\n",
    "\n",
    "LINE_COLOR = \"darkgreen\"\n",
    "LINE_WIDTH = 1.0\n",
    "# ──────────────────────────────────\n",
    "\n",
    "def _fmt_day(x, pos):\n",
    "    d = mdates.num2date(x)\n",
    "    try:\n",
    "        return d.strftime(\"%A %-m/%-d\")   # POSIX\n",
    "    except ValueError:\n",
    "        return d.strftime(\"%A %#m/%#d\")   # Windows\n",
    "\n",
    "def _select_subset(df):\n",
    "    if SUBSET_STYLE == \"window\":\n",
    "        lo = pd.Timestamp(SUBSET_START); hi = pd.Timestamp(SUBSET_END)\n",
    "        m = (df[\"timestamp\"] >= lo) & (df[\"timestamp\"] <= hi)\n",
    "        return df.loc[m].copy(), f\"time window {lo} → {hi}\"\n",
    "    elif SUBSET_STYLE == \"around\":\n",
    "        c = pd.Timestamp(CENTER_TIME)\n",
    "        lo = c - pd.Timedelta(minutes=RADIUS_MIN); hi = c + pd.Timedelta(minutes=RADIUS_MIN)\n",
    "        m = (df[\"timestamp\"] >= lo) & (df[\"timestamp\"] <= hi)\n",
    "        return df.loc[m].copy(), f\"{RADIUS_MIN} min around {c}  (window {lo} → {hi})\"\n",
    "    else:\n",
    "        raise ValueError(\"SUBSET_STYLE must be 'window' or 'around'\")\n",
    "\n",
    "if df.empty:\n",
    "    print(\"Cleaned DataFrame `df` is empty — nothing to plot.\")\n",
    "else:\n",
    "    if PLOT_SCOPE == \"all\":\n",
    "        plot_df = df.copy(); scope_desc = \"ALL cleaned data\"\n",
    "    else:\n",
    "        plot_df, scope_desc = _select_subset(df)\n",
    "        if plot_df.empty:\n",
    "            print(f\"Subset selection produced 0 rows ({scope_desc}). Nothing to plot.\")\n",
    "            print(\"Data range:\", df['timestamp'].min(), \"→\", df['timestamp'].max(), f\"(rows={len(df)})\")\n",
    "        else:\n",
    "            cols_to_show = [c for c in [\"timestamp\",\"value\",\"mode\",\"error\",\"source_file\",\"vbat_mV\",\"vin_mV\",\"iout_mA\",\"soc_C\",\"rp1_C\",\"pmic_C\"] if c in plot_df.columns]\n",
    "            print(f\"\\nSubset rows selected ({len(plot_df)} rows) — {scope_desc}\")\n",
    "            if len(plot_df) > PRINT_MAX_ROWS:\n",
    "                print(plot_df[cols_to_show].head(PRINT_MAX_ROWS).to_string(index=False))\n",
    "                print(f\"... ({len(plot_df) - PRINT_MAX_ROWS} more rows not shown)\")\n",
    "            else:\n",
    "                print(plot_df[cols_to_show].to_string(index=False))\n",
    "            if SAVE_SUBSET_CSV:\n",
    "                plot_df.to_csv(SUBSET_CSV_PATH, index=False)\n",
    "                print(f\"Saved subset to CSV: {SUBSET_CSV_PATH}\")\n",
    "\n",
    "    if not plot_df.empty:\n",
    "        plot_df = plot_df.sort_values(\"timestamp\")\n",
    "        watts_all = plot_df[[\"timestamp\",\"value\"]].rename(columns={\"value\":\"watts\"}).dropna()\n",
    "\n",
    "        sunrise_t = dt.datetime.strptime(SUNRISE_STR, \"%H:%M:%S\").time()\n",
    "        sunset_t  = dt.datetime.strptime(SUNSET_STR,  \"%H:%M:%S\").time()\n",
    "\n",
    "        fig = plt.figure(figsize=FIGSIZE_IN, dpi=DPI)\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        left_data, right_data = watts_all[\"timestamp\"].min(), watts_all[\"timestamp\"].max()\n",
    "\n",
    "        # Major day ticks (LEFT-ALIGNED labels), hourly minor grid\n",
    "        ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(_fmt_day))\n",
    "        ax.xaxis.set_minor_locator(mdates.HourLocator(interval=1))\n",
    "        ax.grid(which=\"minor\", axis=\"x\", linestyle=\"-\", linewidth=0.5, alpha=0.3, color=\"0.8\")\n",
    "\n",
    "        # LEFT-align the major tick labels\n",
    "        for lbl in ax.get_xticklabels(which=\"major\"):\n",
    "            lbl.set_ha(\"left\")\n",
    "        ax.tick_params(axis=\"x\", which=\"major\", pad=6)\n",
    "        ax.margins(x=0.01)  # tiny x-padding so the last left-aligned label isn't clipped\n",
    "\n",
    "        # Night shading\n",
    "        day_start = left_data.normalize()\n",
    "        day_end   = (right_data.normalize() + pd.Timedelta(days=1))\n",
    "        current = day_start\n",
    "        while current < day_end:\n",
    "            sr = pd.Timestamp.combine(current, sunrise_t)\n",
    "            ss = pd.Timestamp.combine(current, sunset_t)\n",
    "            l1, r1 = max(current, left_data), min(sr, right_data)\n",
    "            l2, r2 = max(ss, left_data), min(current + pd.Timedelta(days=1), right_data)\n",
    "            if l1 < r1: ax.axvspan(l1, r1, alpha=0.12, zorder=0)\n",
    "            if l2 < r2: ax.axvspan(l2, r2, alpha=0.12, zorder=0)\n",
    "            current += pd.Timedelta(days=1)\n",
    "\n",
    "        # Generator-on overlay and segmented power lines\n",
    "        spans_df = None\n",
    "        try:\n",
    "            spans_df = log_spans_cut.copy()\n",
    "        except NameError:\n",
    "            try:\n",
    "                spans_df = log_spans.copy()\n",
    "            except NameError:\n",
    "                spans_df = None\n",
    "\n",
    "        if spans_df is not None and not spans_df.empty:\n",
    "            spans_view = spans_df.copy()\n",
    "            spans_view[\"start\"] = spans_view[\"start\"].clip(lower=left_data)\n",
    "            spans_view[\"end\"]   = spans_view[\"end\"].clip(upper=right_data)\n",
    "            spans_view = spans_view[spans_view[\"start\"] < spans_view[\"end\"]]\n",
    "\n",
    "            for _, r in spans_view.iterrows():\n",
    "                m = (watts_all[\"timestamp\"] >= r[\"start\"]) & (watts_all[\"timestamp\"] <= r[\"end\"])\n",
    "                seg = watts_all.loc[m]\n",
    "                if len(seg) >= 2:\n",
    "                    ax.plot(seg[\"timestamp\"], seg[\"watts\"],\n",
    "                            linestyle=\"-\", marker=\"o\", markersize=0,\n",
    "                            linewidth=LINE_WIDTH, color=LINE_COLOR, zorder=3)\n",
    "                ax.hlines(y=0, xmin=r[\"start\"], xmax=r[\"end\"],\n",
    "                          linewidth=3, color=\"red\", alpha=0.7, zorder=2)\n",
    "        else:\n",
    "            ax.plot(watts_all[\"timestamp\"], watts_all[\"watts\"],\n",
    "                    linestyle=\"-\", marker=\"o\", markersize=0,\n",
    "                    linewidth=LINE_WIDTH, color=LINE_COLOR, zorder=3)\n",
    "\n",
    "        ax.set_ylabel(\"Watts\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_title(f\"Watts vs Time — {scope_desc}\")\n",
    "\n",
    "        ymin, ymax = ax.get_ylim()\n",
    "        ax.set_ylim(bottom=min(0, ymin))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_PNG, dpi=DPI)\n",
    "        plt.close(fig)\n",
    "        print(f\"\\nSaved {OUT_PNG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4811c16-d05c-4af4-a060-c81120f227cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
